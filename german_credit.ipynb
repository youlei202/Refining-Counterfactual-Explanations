{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# To prevent automatic figure display when execution of the cell ends\n",
    "%config InlineBackend.close_figures=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from models.mlp import BlackBoxModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display,clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import GermanCreditDataset\n",
    "from experiment import GermanCreditExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "german_credit = GermanCreditDataset()\n",
    "\n",
    "df = german_credit.get_dataframe()\n",
    "\n",
    "df_X, df_y = german_credit.get_Xy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = german_credit.get_standardized_train_test_split(random_state=32)\n",
    "\n",
    "german_credit_experiment = GermanCreditExperiment()\n",
    "german_credit_experiment.train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "for model_name in german_credit_experiment.model_names:\n",
    "    print(\n",
    "        model_name + ':', \n",
    "        german_credit_experiment.model_reports[\n",
    "            german_credit_experiment.model_names[0]\n",
    "        ]['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from explainers import pshap\n",
    "from utils.benchmarking import *\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 100\n",
    "max_round = 50\n",
    "ce_max_iter = 50\n",
    "ci_factor = 1.96  # 95% confidence interval factor\n",
    "n_proj = 10\n",
    "delta = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec52d2817a24e849316b883e6b0691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_name = german_credit.target_name\n",
    "model = german_credit_experiment.models[0]\n",
    "\n",
    "X_test_ext = X_test.copy()\n",
    "X_test_ext[target_name] = model.predict_proba(X_test.values)[:,0]\n",
    "\n",
    "df_baseline = X_test[X_test_ext[target_name] > 0.9]\n",
    "df_explain = X_test\n",
    "\n",
    "max_len = min(df_baseline.shape[0], df_explain.shape[0], sample_num)\n",
    "\n",
    "df_baseline = df_baseline.sample(max_len)\n",
    "df_explain = df_explain.sample(int(max_len))\n",
    "\n",
    "X_baseline = df_baseline.values\n",
    "y_baseline = model.predict_proba(X_baseline)[:,0]\n",
    "X_explain = df_explain.values\n",
    "y_explain = model.predict_proba(X_explain)[:,0]\n",
    "\n",
    "ot_cost = ot.dist(X_explain, X_baseline)\n",
    "matrix_mu = ot.emd(\n",
    "    np.ones(X_explain.shape[0])/X_explain.shape[0], \n",
    "    np.ones(X_baseline.shape[0])/X_baseline.shape[0], ot_cost\n",
    ")\n",
    "\n",
    "shap_explainer = shap.KernelExplainer(lambda X: model.predict_proba(X)[:,0], X_baseline)\n",
    "jp_explainer = pshap.JointProbabilityExplainer(model)\n",
    "# jp_explainer = shap.KernelExplainer(lambda X: model.predict_proba(X)[:,0], X_train.sample(max_len))\n",
    "\n",
    "shap_values_baseline = shap_explainer.shap_values(X_explain)\n",
    "shap_values_jp = jp_explainer.shap_values(X_explain, X_baseline, joint_probs=matrix_mu)\n",
    "# shap_values_jp = jp_explainer.shap_values(X_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c70377055c433d98547f74a18600b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_pairs_list = [25, 50, 75, 100, 150, 200, 300, 400, 500]\n",
    "\n",
    "# Initialize interactive output display\n",
    "plt.ioff()\n",
    "out = widgets.Output()\n",
    "vbox = widgets.VBox([out])\n",
    "display(vbox)\n",
    "\n",
    "# Lists to store accuracies over iterations\n",
    "ot_list_bs = []\n",
    "ot_list_jp = []\n",
    "exp_list_bs = []\n",
    "exp_list_jp = []\n",
    "mmd_list_bs = []\n",
    "mmd_list_jp = []\n",
    "\n",
    "for t in range(max_round):\n",
    "\n",
    "    ot_start, _ = WassersteinDivergence().distance(\n",
    "        torch.FloatTensor(y_explain), \n",
    "        torch.FloatTensor(y_baseline),\n",
    "        delta=delta,\n",
    "        )\n",
    "    kl_start = compute_kl_divergence(\n",
    "            y_explain, \n",
    "            y_baseline,\n",
    "        )\n",
    "    mmd_start = compute_mmd(\n",
    "            y_explain, \n",
    "            y_baseline,\n",
    "        )\n",
    "    # results = [{\n",
    "    #     'OT_bs': ot_start, 'OT_jp': ot_start,\n",
    "    #     'KL_bs': kl_start, 'KL_jp': kl_start,\n",
    "    #     'MMD_bs': mmd_start, 'MMD_jp': mmd_start,\n",
    "    #     }]\n",
    "    results = []\n",
    "    \n",
    "    for num_pairs in num_pairs_list:\n",
    "        result = counterfactual_ability_performance_benchmarking(\n",
    "                model=model,\n",
    "                df_explain=df_explain,\n",
    "                df_baseline=df_baseline,\n",
    "                y_baseline=y_baseline,\n",
    "                shap_values_baseline=shap_values_baseline,\n",
    "                shap_values_jp=shap_values_jp,\n",
    "                num_pairs=num_pairs,\n",
    "                delta=delta,\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    new_ot_bs = [result['OT_bs'] for result in results]\n",
    "    new_ot_jp = [result['OT_jp'] for result in results]\n",
    "\n",
    "    new_exp_bs = [result['EXP_bs'] for result in results]\n",
    "    new_exp_jp = [result['EXP_jp'] for result in results]\n",
    "\n",
    "    new_mmd_bs = [result['MMD_bs'] for result in results]\n",
    "    new_mmd_jp = [result['MMD_jp'] for result in results]\n",
    "\n",
    "    ot_list_bs.append(new_ot_bs)\n",
    "    ot_list_jp.append(new_ot_jp)\n",
    "\n",
    "    exp_list_bs.append(new_exp_bs)\n",
    "    exp_list_jp.append(new_exp_jp)\n",
    "\n",
    "    mmd_list_bs.append(new_mmd_bs)\n",
    "    mmd_list_jp.append(new_mmd_jp)\n",
    "\n",
    "    # Compute mean and confidence intervals for OT\n",
    "    ot_means_bs = np.mean(ot_list_bs, axis=0)\n",
    "    ot_means_jp = np.mean(ot_list_jp, axis=0)\n",
    "    ot_std_err_bs = stats.sem(ot_means_bs, axis=0)\n",
    "    ot_std_err_jp = stats.sem(ot_means_jp, axis=0)\n",
    "    ot_ci_bs = ot_std_err_bs * ci_factor / np.sqrt(t+1)\n",
    "    ot_ci_jp = ot_std_err_jp * ci_factor / np.sqrt(t+1)\n",
    "\n",
    "    # Compute mean and confidence intervals for KL\n",
    "    exp_means_bs = np.mean(exp_list_bs, axis=0)\n",
    "    exp_means_jp = np.mean(exp_list_jp, axis=0)\n",
    "    exp_std_err_bs = stats.sem(exp_means_bs, axis=0)\n",
    "    exp_std_err_jp = stats.sem(exp_means_jp, axis=0)\n",
    "    exp_ci_bs = exp_std_err_bs * ci_factor / np.sqrt(t+1)\n",
    "    exp_ci_jp = exp_std_err_jp * ci_factor / np.sqrt(t+1)\n",
    "\n",
    "    # Compute mean and confidence intervals for MMD\n",
    "    mmd_means_bs = np.mean(mmd_list_bs, axis=0)\n",
    "    mmd_means_jp = np.mean(mmd_list_jp, axis=0)\n",
    "    mmd_std_err_bs = stats.sem(mmd_means_bs, axis=0)\n",
    "    mmd_std_err_jp = stats.sem(mmd_means_jp, axis=0)\n",
    "    mmd_ci_bs = mmd_std_err_bs * ci_factor / np.sqrt(t+1)\n",
    "    mmd_ci_jp = mmd_std_err_jp * ci_factor / np.sqrt(t+1)\n",
    "\n",
    "    fig, axes = plt.subplots(1,3,figsize=(16, 4))\n",
    "    x_labels =  num_pairs_list\n",
    "\n",
    "    # Plotting code for OT Distance\n",
    "    axes[0].plot(x_labels, ot_means_bs, label='SHAP', marker='o')\n",
    "    axes[0].fill_between(x_labels, ot_means_bs - ot_ci_bs, ot_means_bs + ot_ci_bs, alpha=0.2)\n",
    "    axes[0].plot(x_labels, ot_means_jp, label='JP-SHAP', marker='o')\n",
    "    axes[0].fill_between(x_labels, ot_means_jp - ot_ci_jp, ot_means_jp + ot_ci_jp, alpha=0.2)\n",
    "    axes[0].set_xlabel('Number of Changes')\n",
    "    axes[0].set_ylabel('OT Distance')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plotting code for MMD Divergence\n",
    "    axes[1].plot(x_labels, mmd_means_bs, label='SHAP', marker='o')\n",
    "    axes[1].fill_between(x_labels, mmd_means_bs - mmd_ci_bs, mmd_means_bs + mmd_ci_bs, alpha=0.2)\n",
    "    axes[1].plot(x_labels, mmd_means_jp, label='JP-SHAP', marker='o')\n",
    "    axes[1].fill_between(x_labels, mmd_means_jp - mmd_ci_jp, mmd_means_jp + mmd_ci_jp, alpha=0.2)\n",
    "    axes[1].set_xlabel('Number of Changes')\n",
    "    axes[1].set_ylabel('MMD')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # Plotting code for MMD Divergence\n",
    "    axes[2].plot(x_labels, exp_means_bs, label='SHAP', marker='o')\n",
    "    axes[2].fill_between(x_labels, exp_means_bs - exp_ci_bs, exp_means_bs + exp_ci_bs, alpha=0.2)\n",
    "    axes[2].plot(x_labels, exp_means_jp, label='JP-SHAP', marker='o')\n",
    "    axes[2].fill_between(x_labels, exp_means_jp - exp_ci_jp, exp_means_jp + exp_ci_jp, alpha=0.2)\n",
    "    axes[2].set_xlabel('Number of Changes')\n",
    "    axes[2].set_ylabel('Exp Diff')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    # Adjust the spacing between the plots\n",
    "    fig.subplots_adjust(wspace=0.3)  # Increase the width space\n",
    "\n",
    "    with out:\n",
    "        clear_output(wait=True);\n",
    "        print(f'Round:{t}')\n",
    "        display(fig)\n",
    "\n",
    "    plt.close(fig)  # Close the figure to free memory and avoid unnecessary resource use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
